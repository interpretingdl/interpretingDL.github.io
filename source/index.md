---
title: 'Interpreting Deep Learning Models for Text and Sound: Methods & Applications'
layout: single
permalink: /
header:
  overlay_color: "#5e616c"
  overlay_image: /assets/images/network-bw.gif
excerpt: >
  What are the best methods for interpreting the solutions learned by modern deep learning algorithms in language modelling and related domains?
  How can these methods be fruitfully applied to improve the technology, to ensure ethical outcomes and to allow users to adapt them to their needs?
sidebar:
  - text: "The initiators of the _InterpretingDL_ project are
  Willem Zuidema (University of Amsterdam),
  Grzegorz Chrupa≈Ça (Tilburg University),
  Afra Alishahi (Tilburg University),
  Arianna Bisazza (Leiden University),
  Antske Fokkens (Free University Amsterdam),
  Ashley Burgoyne (University of Amsterdam),
  Iris Hendrickx (Radboud University),
  Tom Lentz (University of Amsterdam), and
  Louis ten Bosch (Radboud University)."
---

The combination of Deep Learning and Big Data has revolutionized language and speech technology in the last 5 years, and constitutes the state of the art in domains ranging from machine translation and question-answering to speech recognition and music generation. These models are now often so accurate, that many new useful applications are found with potentially significant impact on individuals, businesses and society. With that power and popularity, new responsibilities and questions arise: how do we ensure reliability, avoid undesirable biases, and provide insights into how a system arrives at a particular outcome? How do we leverage domain expertise and user feedback to improve the models even further? In all these issues, "interpretability" of the deep learning models is key.

In the proposed project, pioneering researchers in the domain of interpretability of deep learning models of text, language, speech and music come together. They collaborate with companies and non-for-profit institutions working with language, speech and music technology, to develop applications that help assess the usefulness of alternative interpretability techniques on a range of different tasks.

In "justification" tasks, we look at how interpretability techniques help give users meaningful feedback. Examples include legal and medical document text mining and audio search. In "augmentation" tasks we look at how these techniques facilitate the use of domain knowledge and models from outside deep learning to make the models perform even better. Examples include machine translation, music recommendation and writing feedback. In "interaction" tasks we allow users to influence the functioning of their automated systems, by providing both interpretable information on how the system operates, and letting human produced output find its way into the internal states of the learning algorithm. Examples include adapting speech recognition to non-standard accents and dialects, interactive music generation, and machine assisted translation.
